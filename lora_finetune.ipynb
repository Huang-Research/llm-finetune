{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HOME=/research/huang/workspaces/hytopot/faultdiagnosis/.hf\n"
     ]
    }
   ],
   "source": [
    "%env HF_HOME = /research/huang/workspaces/hytopot/faultdiagnosis/.hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import RobertaTokenizer, GPTBigCodeForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, set_peft_model_state_dict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"deepseek-ai/deepseek-coder-6.7b-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.HEDataset at 0x7f5fd61d5a50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HEDataset(Dataset):\n",
    "    def __init__(self, path, split):\n",
    "        self.num_classes = 1\n",
    "        self.split = split\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        self.data = self.load_data(path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def load_data(self, path):\n",
    "        data = []\n",
    "\n",
    "        df = pd.read_pickle(path)\n",
    "        df = df[df['split'] == self.split]\n",
    "        # df = df[df['label'].apply(lambda x: len(x) > 0)]\n",
    "        df['label'] = df['label'].apply(lambda x: [0] if 1 in x else [])\n",
    "\n",
    "        input_ids, att_mask = self.tokenizer(df['text'].to_list(), padding='max_length', max_length=2048, truncation=True, return_tensors='pt').values()\n",
    "\n",
    "        label_freqs = df['label'].explode().value_counts().sort_index()\n",
    "        label_freqs /= label_freqs.sum()\n",
    "        self.wts = torch.FloatTensor(label_freqs.to_list())\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            target = torch.zeros(self.num_classes)\n",
    "            for l in df['label'].iloc[i]:\n",
    "                target[l] = 1\n",
    "            data.append({\n",
    "                'input_ids': input_ids[i],\n",
    "                'attention_mask': att_mask[i],\n",
    "                'target': target\n",
    "            })\n",
    "\n",
    "        return data\n",
    "    \n",
    "HEDataset('multi_combined_data.pkl', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.train_dataset = HEDataset('multi_combined_data.pkl', split='train')\n",
    "        self.val_dataset = HEDataset('multi_combined_data.pkl', split='val')\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "        self.device = 'cuda'\n",
    "        # self.encoder = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=self.train_dataset.num_classes).to(self.device)\n",
    "        self.num_epochs = 20\n",
    "\n",
    "        self.tokenizer = self.train_dataset.tokenizer\n",
    "\n",
    "        config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r = 8,\n",
    "            lora_alpha = 32,\n",
    "            lora_dropout = 0.05,\n",
    "            target_modules=\"all-linear\",\n",
    "            task_type=\"SEQ_CLS\",\n",
    "            inference_mode=False,\n",
    "        )\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path,\n",
    "                                                    device_map=\"auto\",\n",
    "                                                    trust_remote_code=False,\n",
    "                                                    revision=\"main\",\n",
    "                                                    num_labels=self.train_dataset.num_classes,\n",
    "                                                    quantization_config=config,\n",
    "                                                    pad_token_id=self.tokenizer.pad_token_id)\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        self.model = get_peft_model(model, lora_config)\n",
    "        self.model.print_trainable_parameters()\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "        self.train_metrics = defaultdict(list)\n",
    "        self.val_metrics = defaultdict(list)\n",
    "\n",
    "        num_training_steps = len(self.train_dataset) * self.num_epochs\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=num_training_steps * 0.06,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "\n",
    "    def train(self):\n",
    "        for self.epoch in range(self.num_epochs):\n",
    "            self.train_one_epoch()\n",
    "\n",
    "            if self.epoch % 1 == 0:\n",
    "                self.validate()\n",
    "\n",
    "                if max(self.val_metrics['loss']) == self.val_metrics['loss'][-1]:\n",
    "                    print('NOT Saving best model...')\n",
    "                    # torch.save(self.model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        losses = []\n",
    "        accs = []\n",
    "        tqdm_batch = tqdm(self.train_loader, total=len(self.train_loader), desc=f'Train [Epoch {self.epoch}]')\n",
    "        for batch in tqdm_batch:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss, acc, preds = self.forward(batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            all_preds.append(preds.detach().cpu().numpy())\n",
    "            all_targets.append(batch['target'].cpu().numpy())\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc)\n",
    "            clear_output(wait=True)\n",
    "            tqdm_batch.set_postfix({'loss': np.mean(losses), 'acc': np.mean(accs)})\n",
    "            print(classification_report(np.concatenate(all_targets), np.concatenate(all_preds), zero_division=np.nan))\n",
    "\n",
    "        self.train_metrics['loss'].append(np.mean(losses))\n",
    "        self.train_metrics['acc'].append(np.mean(accs))\n",
    "        self.train_metrics['report_dict'].append(classification_report(np.concatenate(all_targets), np.concatenate(all_preds), zero_division=0, output_dict=True))\n",
    "        self.train_metrics['report'].append(classification_report(np.concatenate(all_targets), np.concatenate(all_preds), zero_division=np.nan))\n",
    "        self.train_metrics['lr'].append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        losses = []\n",
    "        accs = []\n",
    "        tqdm_batch = tqdm(self.val_loader, total=len(self.val_loader), desc='Val')\n",
    "        for batch in tqdm_batch:\n",
    "\n",
    "            loss, acc, preds = self.forward(batch)\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_targets.append(batch['target'].cpu().numpy())\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc)\n",
    "            clear_output(wait=True)\n",
    "            tqdm_batch.set_postfix({'loss': np.mean(losses), 'acc': np.mean(accs)})\n",
    "            print(classification_report(np.concatenate(all_targets), np.concatenate(all_preds), zero_division=np.nan))\n",
    "        self.val_metrics['loss'].append(np.mean(losses))\n",
    "        self.val_metrics['acc'].append(np.mean(accs))\n",
    "        self.val_metrics['report_dict'].append(classification_report(np.concatenate(all_targets), np.concatenate(all_preds), zero_division=0, output_dict=True))\n",
    "        self.val_metrics['report'].append(classification_report(np.concatenate(all_targets), np.concatenate(all_preds), zero_division=np.nan))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        attention_mask = batch['attention_mask'].to(self.device)\n",
    "        target = batch['target'].to(self.device)\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        preds = torch.sigmoid(logits).round()\n",
    "        acc = (preds == target).sum().item() / target.numel()\n",
    "        loss = self.compute_loss_multilabel(logits, target)\n",
    "        return loss, acc, preds\n",
    "\n",
    "    def compute_loss_multilabel(self, logits, target):\n",
    "        # target is onehot\n",
    "        return F.binary_cross_entropy_with_logits(torch.sigmoid(logits), target, weight=self.train_dataset.wts.to(self.device))\n",
    "\n",
    "    def dump_val_preds(self):\n",
    "        np.set_printoptions(precision=4, suppress=True)\n",
    "        torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        preds = []\n",
    "        targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                target = batch['target'].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds.append(torch.sigmoid(logits).round().cpu().numpy())\n",
    "                targets.append(target.cpu().numpy())\n",
    "        for p, t in zip(preds, targets):\n",
    "            print(p.squeeze(0), t.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train [Epoch 0]:  13%|█▎        | 33/252 [10:23<1:09:53, 19.15s/it, loss=0.711, acc=0.545]/research/huang/workspaces/hytopot/miniconda3/envs/program1/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      1.00      0.68        16\n",
      "         1.0       1.00      0.12      0.21        17\n",
      "\n",
      "    accuracy                           0.55        33\n",
      "   macro avg       0.76      0.56      0.45        33\n",
      "weighted avg       0.77      0.55      0.44        33\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train [Epoch 0]:  13%|█▎        | 33/252 [10:42<1:11:04, 19.47s/it, loss=0.711, acc=0.545]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m a \u001b[38;5;241m=\u001b[39m Agent()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(a\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[0;32m---> 10\u001b[0m     \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     a\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m     12\u001b[0m     clear_output()\n",
      "Cell \u001b[0;32mIn[5], line 92\u001b[0m, in \u001b[0;36mAgent.train_one_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m loss, acc, preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch)\n\u001b[1;32m     90\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 92\u001b[0m all_preds\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     93\u001b[0m all_targets\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "a = Agent()\n",
    "\n",
    "for a.epoch in range(a.num_epochs):\n",
    "    a.train_one_epoch()\n",
    "    a.validate()\n",
    "    clear_output()\n",
    "\n",
    "    ax, fig = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax2 = fig[0].twinx()\n",
    "    ax2.plot(a.train_metrics['lr'], color='black', alpha=0.3)\n",
    "    ax2.set_ylabel('lr')\n",
    "    fig[0].set_xlim(0, a.num_epochs * len(a.train_loader))\n",
    "    fig[0].plot(a.train_metrics['loss'], label='train')\n",
    "    fig[0].plot(a.val_metrics['loss'], label='val')\n",
    "    fig[0].set_title('Loss')\n",
    "    fig[0].legend()\n",
    "\n",
    "    fig[1].set_xlim(0, a.num_epochs * len(a.train_loader))\n",
    "    fig[1].set_ylim(0, 1)\n",
    "    # fig[1].plot(list(map(lambda x: x['0']['f1-score'], a.train_metrics['report_dict'])), c='lightred')\n",
    "    # fig[1].plot(list(map(lambda x: x['1']['f1-score'], a.train_metrics['report_dict'])), c='red')\n",
    "    # fig[1].plot(list(map(lambda x: x['2']['f1-score'], a.train_metrics['report_dict'])), c='darkred')\n",
    "    # fig[1].plot(list(map(lambda x: x['0']['f1-score'], a.train_metrics['report_dict'])), c='lightblue')\n",
    "    # fig[1].plot(list(map(lambda x: x['1']['f1-score'], a.train_metrics['report_dict'])), c='blue')\n",
    "    # fig[1].plot(list(map(lambda x: x['2']['f1-score'], a.train_metrics['report_dict'])), c='darkblue')\n",
    "    # fig[1].plot(list(map(lambda x: x['0']['precision'], a.train_metrics['report_dict'])), c='r')\n",
    "    # fig[1].plot(list(map(lambda x: x['0']['recall'], a.train_metrics['report_dict'])), c='darkred')\n",
    "    # fig[1].plot(list(map(lambda x: x['1']['precision'], a.train_metrics['report_dict'])), c='g')\n",
    "    # fig[1].plot(list(map(lambda x: x['1']['recall'], a.train_metrics['report_dict'])), c='darkgreen')\n",
    "    # fig[1].plot(list(map(lambda x: x['2']['precision'], a.train_metrics['report_dict'])), c='b')\n",
    "    # fig[1].plot(list(map(lambda x: x['2']['recall'], a.train_metrics['report_dict'])), c='darkblue')\n",
    "    # fig[1].plot(list(map(lambda x: x['0']['precision'], a.val_metrics['report_dict'])), c='r', linestyle='--')\n",
    "    # fig[1].plot(list(map(lambda x: x['0']['recall'], a.val_metrics['report_dict'])), c='darkred', linestyle='--')\n",
    "    # fig[1].plot(list(map(lambda x: x['1']['precision'], a.val_metrics['report_dict'])), c='g', linestyle='--')\n",
    "    # fig[1].plot(list(map(lambda x: x['1']['recall'], a.val_metrics['report_dict'])), c='darkgreen', linestyle='--')\n",
    "    # fig[1].plot(list(map(lambda x: x['2']['precision'], a.val_metrics['report_dict'])), c='b', linestyle='--')\n",
    "    # fig[1].plot(list(map(lambda x: x['2']['recall'], a.val_metrics['report_dict'])), c='darkblue', linestyle='--')\n",
    "    fig[1].plot(a.train_metrics['acc'], label='train')\n",
    "    fig[1].plot(a.val_metrics['acc'], label='val')\n",
    "    fig[1].set_title('Metrics')\n",
    "    # lines = [Line2D([0], [0], color='black'), Line2D([0], [0], color='black', linestyle='--'),\n",
    "    #         Line2D([0], [0], color='red'), Line2D([0], [0], color='darkred')]\n",
    "    # fig[1].legend(lines, ['Train', 'Val', 'Precision', 'Recall'])\n",
    "    plt.show()\n",
    " \n",
    "    print(f\"Train report { ' ' * (len(a.train_metrics['report'][-1]) - 12) } Val report\")\n",
    "    for line in zip(a.train_metrics['report'][-1].split('\\n'), a.val_metrics['report'][-1].split('\\n')):\n",
    "        print(line[0], line[1])\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
